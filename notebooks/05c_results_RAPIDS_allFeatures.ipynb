{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChadDelany/drought_prediction/blob/main/notebooks/05b_Results_RAPIDS_allFeatures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Drought Prediction: Results**"
      ],
      "metadata": {
        "id": "9mR2bPvswIvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was run on Google Colab Pro+.  The RAPIDS system was used to access GPU processing.  Previous attempts to run the models using Pandas with CPUs took 24+ hours to run and often crashed due to exceeding existing resources.  The RAPIDS GPU processing allowed models to run usually within 5 minutes and at a maximum of 15 minutes.  Setting up RAPIDS to run on Google Colab Pro+ takes between 15 minutes to 1 hour depending on resource availability on Google Colab Pro+.\n",
        "\n",
        "ALL MODELS INITIALLY RUN WITH ALL AVAILABLE VARIABLES TO DETERMINE INITIAL MODEL PERFORMANCE."
      ],
      "metadata": {
        "id": "L8B6xerQwatO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIS3fhPT8naz"
      },
      "source": [
        "# Environment Sanity Check #\n",
        "\n",
        "Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n",
        "\n",
        "Check the output of `!nvidia-smi` to make sure you've been allocated a Tesla T4, P4, or P100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTUcQ76Wb3NA",
        "outputId": "9b38ba1b-f47d-4844-a056-dda466548892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec 12 19:29:03 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    55W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfVoeLbP8iZp"
      },
      "source": [
        "#Setup:\n",
        "This notebook was built on RAPIDS 0.13 stable and is based on this [DataCamp Tutorial](https://www.datacamp.com/community/tutorials/xgboost-in-python).  tested and working on 0.19 stable.\n",
        "\n",
        "#Setup:\n",
        "Set up script installs\n",
        "1. Updates gcc in Colab\n",
        "1. Installs Conda\n",
        "1. Install RAPIDS' current stable version of its libraries, as well as some external libraries including:\n",
        "  1. cuDF\n",
        "  1. cuML\n",
        "  1. cuGraph\n",
        "  1. cuSpatial\n",
        "  1. cuSignal\n",
        "  1. BlazingSQL\n",
        "  1. xgboost\n",
        "1. Copy RAPIDS .so files into current working directory, a neccessary workaround for RAPIDS+Colab integration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH3gMCzX3siM",
        "outputId": "356400fa-12fe-490f-c6c6-4c58fe8d43fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pynvml\n",
            "  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46 kB 831 kB/s \n",
            "\u001b[?25hInstalling collected packages: pynvml\n",
            "Successfully installed pynvml-11.4.1\n",
            "Cloning into 'rapidsai-csp-utils'...\n",
            "remote: Enumerating objects: 308, done.\u001b[K\n",
            "remote: Counting objects: 100% (137/137), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 308 (delta 79), reused 98 (delta 55), pack-reused 171\u001b[K\n",
            "Receiving objects: 100% (308/308), 89.88 KiB | 22.47 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n",
            "***********************************************************************\n",
            "Woo! Your instance has the right kind of GPU, a A100-SXM4-40GB!\n",
            "***********************************************************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install pynvml\n",
        "\n",
        "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
        "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/env-check.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNd6j0tPahdh",
        "outputId": "7a052331-67c4-4b8e-8a95-e1fd5ceee5d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating your Colab environment.  This will restart your kernel.  Don't Panic!\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [1,073 kB]\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,524 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,567 kB]\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic InRelease [20.8 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,342 kB]\n",
            "Get:19 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 Packages [50.4 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,099 kB]\n",
            "Fetched 11.9 MB in 3s (3,535 kB/s)\n",
            "Reading package lists... Done\n",
            "Added repo\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Installing libstdc++\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Selected version '11.1.0-1ubuntu1~18.04.1' (Toolchain test builds:18.04/bionic [amd64]) for 'libstdc++6'\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  gcc-11-base libgcc-s1\n",
            "The following NEW packages will be installed:\n",
            "  gcc-11-base libgcc-s1\n",
            "The following packages will be upgraded:\n",
            "  libstdc++6\n",
            "1 upgraded, 2 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 641 kB of archives.\n",
            "After this operation, 981 kB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 gcc-11-base amd64 11.1.0-1ubuntu1~18.04.1 [19.0 kB]\n",
            "Get:2 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 libgcc-s1 amd64 11.1.0-1ubuntu1~18.04.1 [41.8 kB]\n",
            "Get:3 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 libstdc++6 amd64 11.1.0-1ubuntu1~18.04.1 [580 kB]\n",
            "Fetched 641 kB in 2s (279 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package gcc-11-base:amd64.\n",
            "(Reading database ... 124013 files and directories currently installed.)\n",
            "Preparing to unpack .../gcc-11-base_11.1.0-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking gcc-11-base:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Setting up gcc-11-base:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Selecting previously unselected package libgcc-s1:amd64.\n",
            "(Reading database ... 124018 files and directories currently installed.)\n",
            "Preparing to unpack .../libgcc-s1_11.1.0-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libgcc-s1:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Replacing files in old package libgcc1:amd64 (1:8.4.0-1ubuntu1~18.04) ...\n",
            "Setting up libgcc-s1:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "(Reading database ... 124020 files and directories currently installed.)\n",
            "Preparing to unpack .../libstdc++6_11.1.0-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libstdc++6:amd64 (11.1.0-1ubuntu1~18.04.1) over (8.4.0-1ubuntu1~18.04) ...\n",
            "Setting up libstdc++6:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "restarting Colab...\n"
          ]
        }
      ],
      "source": [
        "# This will update the Colab environment and restart the kernel.  Don't run the next cell until you see the session crash.\n",
        "!bash rapidsai-csp-utils/colab/update_gcc.sh\n",
        "import os\n",
        "os._exit(00)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_8QCkggahkH",
        "outputId": "67340cfc-4d88-487b-a5db-8a2b9c95b87b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è¨ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "üì¶ Installing...\n",
            "üìå Adjusting configuration...\n",
            "ü©π Patching environment...\n",
            "‚è≤ Done in 0:00:23\n",
            "üîÅ Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "# This will install CondaColab.  This will restart your kernel one last time.  Run this cell by itself and only run the next cell once you see the session crash.\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts_8oCWvahnN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11d6e0a0-5a85-4f0a-dd62-1c4a2c014297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ú®üç∞‚ú® Everything looks OK!\n"
          ]
        }
      ],
      "source": [
        "# you can now run the rest of the cells as normal\n",
        "import condacolab\n",
        "condacolab.check()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-ZgdDIIahqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5e3ea8-cc3a-41e4-fc0b-812e8d611318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: cffi 1.15.1\n",
            "Uninstalling cffi-1.15.1:\n",
            "  Successfully uninstalled cffi-1.15.1\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "Found existing installation: cryptography 38.0.4\n",
            "Uninstalling cryptography-38.0.4:\n",
            "  Successfully uninstalled cryptography-38.0.4\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cffi==1.15.0\n",
            "  Downloading cffi-1.15.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (446 kB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 446.7/446.7 kB 14.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/site-packages (from cffi==1.15.0) (2.21)\n",
            "Installing collected packages: cffi\n",
            "Successfully installed cffi-1.15.0\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "Installing RAPIDS Stable 21.12\n",
            "Starting the RAPIDS install on Colab.  This will take about 15 minutes.\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... WARNING conda.core.solve:_add_specs(652): pinned spec python=3.8 conflicts with explicit specs.  Overriding pinned spec.\n",
            "failed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: ...working... WARNING conda.core.solve:_add_specs(652): pinned spec python=3.8 conflicts with explicit specs.  Overriding pinned spec.\n",
            "failed\n",
            "\n",
            "SpecsConfigurationConflictError: Requested specs conflict with configured specs.\n",
            "  requested specs:\n",
            "    - cudatoolkit=11.2\n",
            "    - dask-sql\n",
            "    - gcsfs\n",
            "    - llvmlite\n",
            "    - mamba==1.0.0\n",
            "    - openssl\n",
            "    - python=3.7\n",
            "    - rapids=21.12\n",
            "  pinned specs:\n",
            "    - python_abi=3.8[build=*cp38*]\n",
            "Use 'conda config --show-sources' to look for 'pinned_specs' and 'track_features'\n",
            "configuration parameters.  Pinned specs may also be defined in the file\n",
            "/usr/local/conda-meta/pinned.\n",
            "\n",
            "\n",
            "RAPIDS conda installation complete.  Updating Colab's libraries...\n"
          ]
        }
      ],
      "source": [
        "# Installing RAPIDS is now 'python rapidsai-csp-utils/colab/install_rapids.py <release> <packages>'\n",
        "# The <release> options are 'stable' and 'nightly'.  Leaving it blank or adding any other words will default to stable.\n",
        "!python rapidsai-csp-utils/colab/install_rapids.py stable\n",
        "import os\n",
        "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
        "os.environ['CONDA_PREFIX'] = '/usr/local'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load RAPIDS libraries"
      ],
      "metadata": {
        "id": "aekPmRDLVPHi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzdJ7SnJ4QDD"
      },
      "outputs": [],
      "source": [
        "# RAPIDS libraries for accessing GPU processing for running models.  Instead of taking 24+ hours to run models, it only takes 15 minutes or less.\n",
        "import cudf\n",
        "import cuml\n",
        "import cupy\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import pynvml\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a28212ae"
      },
      "source": [
        "## Load additional Libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "034d9b70"
      },
      "outputs": [],
      "source": [
        "# Import Sklearn metrics.  The RAPIDS metrics currently appeared bugged.\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "# For displaying model metrics in an easily readable table form.\n",
        "from IPython.display import HTML, display\n",
        "import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "3Smb2w30p9fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Accuracy Assessment for Regression."
      ],
      "metadata": {
        "id": "B_hg_788vzLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for Model Accuracy Assessment for Regression.  Input is in Pandas because RAPIDS metrics currently appeared to be bugged.\n",
        "def reg_metric(y_test, y_pred):\n",
        "  # Calculation of metrics\n",
        "  r2 = str(round(r2_score(y_test, y_pred), 3))\n",
        "  mse = str(round(mean_squared_error(y_test, y_pred), 3))\n",
        "  rmse = str(round(np.sqrt(mean_squared_error(y_test, y_pred)), 3))\n",
        "  mae = str(round(mean_absolute_error(y_test, y_pred), 3))\n",
        "\n",
        "  #create table for display\n",
        "  metric = [['Metric', 'Value'],\n",
        "            ['R**2:', r2],\n",
        "            ['MSE:', mse],\n",
        "            ['RMSE:', rmse],\n",
        "            ['MAE:', mae]]\n",
        "  table = tabulate.tabulate(metric, tablefmt='html')\n",
        "  display(HTML(table))\n",
        "\n",
        "  # Return metric values as strings for later display.\n",
        "  return(r2, mse, rmse, mae)"
      ],
      "metadata": {
        "id": "Ap_uwkW0V54y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coefficients from Regression Models."
      ],
      "metadata": {
        "id": "pVdRLSfFv4EG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to pull coefficients for regression models.\n",
        "def reg_coefs(model):\n",
        "  # get coefficients from RAPIDS model\n",
        "  coefs = model.coef_\n",
        "  coefs = coefs.to_pandas()\n",
        "\n",
        "  # associate variable names with coefficients\n",
        "  features = X_train.columns\n",
        "  \n",
        "  # Create Pandas Series with appropriate labels\n",
        "  coefs = coefs.set_axis(features)\n",
        "\n",
        "  return(coefs)"
      ],
      "metadata": {
        "id": "3V9E3-Yqwy2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Accuracy Assessment for Classification."
      ],
      "metadata": {
        "id": "pNPpcAM8eoWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for calculating Classification Metrics\n",
        "def class_metric(ycat_test, y_pred):\n",
        "  #Calculation of metrics\n",
        "  accuracy = str(np.round(cuml.metrics.accuracy.accuracy_score(ycat_test, y_pred), 3) * 100)\n",
        "  roc_auc = str(np.round(cuml.metrics.roc_auc_score(ycat_test, y_pred), 3))\n",
        "\n",
        "  cp = np.round(cuml.metrics.confusion_matrix(ycat_test, y_pred, normalize='pred'), 3) * 100\n",
        "  pred_perclass = [cp[0][0].get(), cp[1][1].get(), cp[2][2].get(), cp[3][3].get(), cp[4][4].get(), cp[5][5].get()]\n",
        "\n",
        "  cp_mean = str(np.round(np.mean(pred_perclass), 1))\n",
        "  cp_std = str(np.round(np.std(pred_perclass), 1))\n",
        "\n",
        "  print(f'Accuracy Score: {accuracy}%')\n",
        "  print(f'ROC AUC: {roc_auc}')\n",
        "  print(f'Mean Accuracy per Class & Standard Deviation: {cp_mean}% +/- {cp_std}%')\n",
        "  print(cp)\n",
        "\n",
        "  return(accuracy, roc_auc, cp, cp_mean, cp_std)"
      ],
      "metadata": {
        "id": "LaRm3k8HZ1ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset."
      ],
      "metadata": {
        "id": "WEb2mxsqptdQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL8rpsnFj-2Y"
      },
      "outputs": [],
      "source": [
        "# Local location of the data\n",
        "\n",
        "# Location on Windows\n",
        "# local_data = 'D:\\\\Data_Science\\\\DroughtProject\\\\Data\\\\' \n",
        "\n",
        "# Location on Linux\n",
        "# local_data = '/home/chad/Data/Drought_Prediction/' \n",
        "\n",
        "# Load local data into Google Colab\n",
        "# from google.colab import files\n",
        "# files = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqo-yIYjoyMj",
        "outputId": "86168f20-43ae-4140-e858-144f2dabf1b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Accessing Google Drive by mounting it locally\n",
        "# https://towardsdatascience.com/7-ways-to-load-external-data-into-google-colab-7ba73e7d5fc7\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9b4ObRMp5f-"
      },
      "outputs": [],
      "source": [
        "# Location on Google Drive\n",
        "local_data = '/content/drive/MyDrive/Colab Notebooks/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d643f7f"
      },
      "outputs": [],
      "source": [
        "# Load the dataset that contains training (meteorological variables) resampled weekly with mean, max, min\n",
        "# and the soil variables that have been merged on the county 'fips' value\n",
        "# This version of the file has already been scaled for the mean equal to zero and the variance to a standard deviation via StandardScaler.\n",
        "\n",
        "tsm = cudf.read_csv(local_data + 'train_soil_stats_scaled.csv',\n",
        "                        parse_dates=['date'],\n",
        "                        index_col=['index'],\n",
        "                        header=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset that contains training (meteorological variables) resampled weekly with mean, max, min\n",
        "# and the soil variables that have been merged on the county 'fips' value\n",
        "# This version of the file has already been scaled for the mean equal to zero and the variance to a standard deviation via StandardScaler.\n",
        "\n",
        "testval = cudf.read_csv(local_data + 'testval_soil_stats_scaled.csv',\n",
        "                        parse_dates=['date'],\n",
        "                        index_col=['index'],\n",
        "                        header=0)"
      ],
      "metadata": {
        "id": "OWy5PdiQpWF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv1gUz_npT0p"
      },
      "outputs": [],
      "source": [
        "# Unmount Google Drive.\n",
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7572e92"
      },
      "source": [
        "## Select Features and Target for Models using Full Set of Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a1be6d5"
      },
      "outputs": [],
      "source": [
        "# Breaking out independent numerical variables from target variable, categorical variable ('fips'), and date.\n",
        "cols = tsm.columns.tolist()\n",
        "features = cols[3:]\n",
        "\n",
        "# Separating out the features\n",
        "Xtrain = tsm[features]\n",
        "\n",
        "# Separating out the target\n",
        "ytrain = tsm[['score']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feb24f09"
      },
      "outputs": [],
      "source": [
        "# Converting 'y' from Panda Dataframe to Panda Series to avoid conflicts with type when running RAPIDS models.\n",
        "ytrain = ytrain['score']\n",
        "\n",
        "# Convert X from float64 to float32 in order to utilize GPU processing instead of CPU processing.  RAPIDS currently does not support float64.\n",
        "Xtrain = Xtrain.astype('float32')\n",
        "\n",
        "# Convert y from float64 to float32 in order to utilize GPU processing instead of CPU processing.\n",
        "ytrain = ytrain.astype('float32')\n",
        "\n",
        "# Create target for classication models.  Drought Score was originally an integer class ranging 0 - 5.\n",
        "ytrain_cat = np.round(ytrain,0)\n",
        "ytrain_cat = ytrain_cat.astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhWpOecwqf2Y"
      },
      "source": [
        "## Select Features and Target for Models from Final Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVdL9B3mqf2e"
      },
      "outputs": [],
      "source": [
        "# Breaking out independent numerical variables from target variable, categorical variable ('fips'), and date.\n",
        "cols = testval.columns.tolist()\n",
        "features = cols[3:]\n",
        "\n",
        "# Separating out the features\n",
        "Xtest = testval[features]\n",
        "\n",
        "# Separating out the target\n",
        "ytest = testval[['score']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgyVsumYqf2e"
      },
      "outputs": [],
      "source": [
        "# Converting 'y' from Panda Dataframe to Panda Series to avoid conflicts with type when running RAPIDS models.\n",
        "ytest = ytest['score']\n",
        "\n",
        "# Convert X from float64 to float32 in order to utilize GPU processing instead of CPU processing.  RAPIDS currently does not support float64.\n",
        "Xtest = Xtest.astype('float32')\n",
        "\n",
        "# Convert y from float64 to float32 in order to utilize GPU processing instead of CPU processing.\n",
        "ytest = ytest.astype('float32')\n",
        "\n",
        "# Create target for classication models.  Drought Score was originally an integer class ranging 0 - 5.\n",
        "ytest_cat = np.round(ytest,0)\n",
        "ytest_cat = ytest_cat.astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Classification Model"
      ],
      "metadata": {
        "id": "xY_Z7eu6m5RW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest Classifier, max_depth=100, n_estimators=300"
      ],
      "metadata": {
        "id": "0c0O4VzJbREs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Best Verified Model on All Training Data.\n",
        "# Random Forest Classifier Model, MAX_DEPTH = 100, n_estimators = 300\n",
        "RFclass_model = cuml.ensemble.RandomForestClassifier(max_depth=100, n_estimators=300)\n",
        "RFclass_model.fit(Xtrain, ytrain_cat)"
      ],
      "metadata": {
        "id": "6n6dEVC8bREx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a8d09b-c927-4181-c9a1-8c64294c9758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Test on Previously Withheld Test Data.\n",
        "y_pred = RFclass_model.predict(Xtest)\n",
        "y_pred = y_pred.astype(int)"
      ],
      "metadata": {
        "id": "PGXw_cCB7c8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy Assessment for Classification Model.\n",
        "RF_acc, RF_roc, RF_cp, RF_cpMean, RF_cpSTD = class_metric(ytest_cat, y_pred)"
      ],
      "metadata": {
        "id": "r6ZCD50Xg1hz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3129b230-8376-4426-ae70-b7a825a9d4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 73.7%\n",
            "ROC AUC: 0.515\n",
            "Mean Accuracy per Class & Standard Deviation: 18.8% +/- 26.5%\n",
            "[[76.1 68.1 58.1 56.7 61.5 60. ]\n",
            " [12.9 17.2 20.4 23.7 23.   0. ]\n",
            " [ 6.7  9.1 14.6 12.9 11.5  0. ]\n",
            " [ 2.9  3.9  4.4  4.8  4.1 20. ]\n",
            " [ 1.1  1.5  2.2  1.6  0.  20. ]\n",
            " [ 0.3  0.3  0.2  0.2  0.   0. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuruO_yUDInC"
      },
      "source": [
        "# **Conclusions**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Best Model: Random Forest (max_depth=100, n_estimators=300)**"
      ],
      "metadata": {
        "id": "rzayoIyj1Zj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy for each Drought Score Category.\n",
        "metric = [['Class', 'Accuracy'],\n",
        "          ['0', '76.1%'],\n",
        "          ['1', '17.2%'],\n",
        "          ['2', '14.6%'],\n",
        "          ['3', '4.8%'],\n",
        "          ['4', '0%'],\n",
        "          ['5', '0%']]\n",
        "table = tabulate.tabulate(metric, tablefmt='html')\n",
        "\n",
        "display(HTML(table))"
      ],
      "metadata": {
        "id": "qYAmJVa17aIs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "aad56180-1e50-4b26-bdeb-bd0a0b2e6ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table>\n",
              "<tbody>\n",
              "<tr><td>Class</td><td>Accuracy</td></tr>\n",
              "<tr><td>0    </td><td>76.1%   </td></tr>\n",
              "<tr><td>1    </td><td>17.2%   </td></tr>\n",
              "<tr><td>2    </td><td>14.6%   </td></tr>\n",
              "<tr><td>3    </td><td>4.8%    </td></tr>\n",
              "<tr><td>4    </td><td>0%      </td></tr>\n",
              "<tr><td>5    </td><td>0%      </td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTdajjVRDKfC"
      },
      "source": [
        "**Conclusion and Next Steps:**<br> This process did produce a viable model and demonstrated the usefulness of random forests for this problem. It was not able to highlight a few, key variables. The next steps to improve this model would be:\n",
        "\n",
        "- Allocate more resources so that the training can be done on the entire training dataset to exact key variables.\n",
        "- Subset the training dataset and rerun the models to allow for a standard cross validation procedure and allow tools that determine important input variables to be determined.\n",
        "- Incorporate ordinality information into the classification schema. \n",
        "- Incorporate a time series analysis that capitalizes on the time nature of the data.\n",
        "- Use a recurrent neural network to build a time series model.\n",
        "\n",
        "The initial overall accuracy of the Random Forest model is 74%. With an additional allocation of time and resources, these models could absolutely reach an accuracy above 80%. This is especially true when the cardinality of drought scores is incorporated into the models and ever more importantly the information contained within the timeseries. Additionally, a recurrent neural network may be able to leverage deep learning available within such a large dataset. Given the changing climate and the inherent integration of economies throughout the present-day world, understanding and accurately predicting drought is an important first step in adapting to the current changing conditions of our environment and maintaining a viable global economy.  Being able to predict drought from simple variables and not overly complex models, would allow them to be applied worldwide."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
